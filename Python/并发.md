- [GIL锁](#gil锁)
  - [有了GIL锁就是线程安全？](#有了gil锁就是线程安全)
- [协程](#协程)
  - [asyncio](#asyncio)
  - [gevent](#gevent)
- [yeild](#yeild)
- [并发安全](#并发安全)

# GIL锁

GIL锁目的：垃圾回收的引用计数法导致内存

假设有两个线程 A 和 B 同时引用一个对象 a。  
线程 A 开始引用对象 a，此时对象 a 的引用计数为 1。   
线程 B 也开始引用对象 a，此时对象 a 的引用计数应该增加为 2，但由于引用计数操作不是原子性的，因此在增加引用计数的过程中，线程 A 和线程 B 可能同时执行引用计数的增加操作。  
由于引用计数操作不是原子性的，可能会出现这样的情况：线程 A 和线程 B 同时对引用计数进行操作，但由于操作不同步，最终计数只增加了 1，使得对象 a 的引用计数仅增加为 2 而不是 3。  
这种情况下，当线程 A 结束时，会将引用计数减少 1，导致引用计数变为 1。此时如果对象 a 的内存被释放，线程 B 再次访问对象 a 时，就会出现无效的内存访问。

CPython中还有另一个机制，叫做check_interval，意思是CPython解释器会去轮询检查线程GIL的锁住情况。每隔一段时间，Python解释器就会强制当前线程去释放GIL，这样别的线程才能有执行的机会。或者当前的线程阻塞了，会切换

## 有了GIL锁就是线程安全？

肯定不是的，要看操作本身是否原子性，比如 n=n+1 不是原子操作，即使有GIL锁，线程也可能在由于非原子操作导致线程不安全

# 协程

- 协程和多线程的区别，主要在于两点，一是协程为单线程；二是协程由用户决定，在哪些地方交出控制权，切换到下一个任务。
- 协程的写法更加简洁清晰，把async / await 语法和 create_task 结合来用，对于中小级别的并发需求已经毫无压力。
- 写协程程序的时候，你的脑海中要有清晰的事件循环概念，知道程序在什么时候需要暂停、等待 I/O，什么时候需要一并执行到底。

> 协程与 subroutine 不同的是，协程在执行过程中可以暂停，把线程让出（yield）给其它代码去执行，然后在某个时间点再恢复执行，这种 yield 不是调用关系而是切换，线程从一个协程的 Context 切换到了另一个协程的 Context。在这种情况下，多个协程便可以在同一线程下并发执行。也就是说，协程的暂停后再恢复机制，是为了能够让出线程的使用权，而让出线程的使用权后，就可以在当前线程去执行其它协程，也就实现了多个协程共同运行。通常也会认为 subroutine 是 coroutine 的一个特例，subroutine 是在执行中不会让出 (yeild) 的 coroutine。

以 Python 中的 sleep 为例，我们让代码睡眠 5 秒钟后继续执行，我们在函数中调用 time.sleep(5) 时，当前函数停止执行了，当前线程也是停住的，不会执行其它代码。而在协程中调用 asyncio.sleep(5) 时，当前协程停止执行了，但是当前线程仍然会继续运行，去执行其它代码，5 秒钟后当前协程自动恢复执行。需要注意的是，我们在函数中调用的是 time.sleep(5)，**而在协程中调用的是 asyncio.sleep(5)，asyncio.sleep 是协程版本的** sleep 函数。如果在协程中调用 time.sleep(5)，协程停住的5秒钟中，线程同样也是停住的，必须调用协程版本的 asyncio.sleep 才能实现协程的效果。

无论是IO阻塞还是time.sleep这类非IO阻塞。**阻塞想要协程切换，就需要协程。怎样才能有协程：gevent，asyncio等，要想实现协程序切换，需要代码中有协程切换的逻辑，而不是说用了gevent和asnycio就会切换
协程能切换：**

1、传统IO操作/time.sleep + gevnet的monkey.patch,  此时monkey patch会将IO操作/sleep等阻塞打补丁替换

2、传统IO阻塞用异步调用的库asnycio.sleep + asnycio

3、自己写yield实现

## asyncio

实现协程：

（1）事件循环 (event loop)。事件循环需要实现两个功能，一是顺序执行协程代码；二是完成协程的调度，即一个协程“暂停”时，决定接下来执行哪个协程。

（2）协程上下文的切换。基本上Python 生成器的 yeild 已经能完成切换，Python3中还有特定语法await支持协程切换。

```python
import asyncio

async def do_work():
    print("Hello....")
    # 模拟阻塞1秒
    await asyncio.sleep(1)
    print("world...")

coroutine = do_work()
# 创建一个事件event_loop
loop = asyncio.get_event_loop()
# 将协程加入到event_loop中，并运行
loop.run_until_complete(coroutine)
```

## gevent

monkey.patch_all() 的作用是自动化地对 Python 的一些标准库进行修改，使它们变得与 gevent 协作。这包括对time.sleep的修改，使其成为非阻塞的。

```python
import time
import gevent
from gevent import monkey
import requests 
 
# 这一过程在启动时通过monkey patch完成
monkey.patch_all()
 
 
def func_a():
    print('-------A-------')
    # time.sleep(5)
    ret = requests.get("http://www.baidu.com")
    print(ret.status_code)
    print('-------A1-------')
 
 
def func_b():
    print('-------B-------')
    # time.sleep(5)
    ret = requests.get("http://www.baidu.com")
    print(ret.status_code)
    print('-------B1-------')
 
 
# gevent.joinall([gevent.spawn(fn)
g1 = gevent.spawn(func_a)  # 创建一个协程
g2 = gevent.spawn(func_b)
g1.join()  # 等待协程执行结束
g2.join()

gevent.joinall(
    [
        gevent.spawn(func_a),
        gevent.spawn(func_b)
    ]
)
```

# yeild

```python
from collections import deque
 
def sayHello(n):
    while n > 0:
        print("hello~", n)
        yield n
        n -= 1
    print('say hello')
 
def sayHi(n):
    x = 0
    while x < n:
        print('hi~', x)
        yield
        x += 1
    print("say hi")
 
# 使用yield语句，实现简单任务调度器
class TaskScheduler(object):
    def __init__(self):
        self._task_queue = deque()
 
    def new_task(self, task):
        '''
        向调度队列添加新的任务
        '''
        self._task_queue.append(task)
 
    def run(self):
        '''
        不断运行，直到队列中没有任务
        '''
        while self._task_queue:
            task = self._task_queue.popleft()
            try:
                next(task)
                self._task_queue.append(task)
            except StopIteration:
                # 生成器结束
                pass
 
sched = TaskScheduler()
sched.new_task(sayHello(2))
sched.new_task(sayHi(3))
sched.run()　

hello~ 2
hi~ 0
hello~ 1
hi~ 1
say hello
hi~ 2
say hi
```

# 并发安全

- 共享的资源一般需要分布式锁，比如抢票剩余500张
- 进程（副本）比如只要一个RedisClient, 这时候如果加锁就不需要分布式锁，因为每个进程需要一个
- 
下面的单例模式，不是原子操作，会不会有并发问题：

1、多线程模式下，是可能会有的，因为线程是由OS调度的，比如刚好时间片到的时候卡在有原子操作的地方，就会有并发问题

2、协程模式下，不会有，协程的切换控制权在我们手上，此时我们没有进行切换，所以一直是这个协程在运行，且因为GIL锁的原因，此时只有一个协程运行，所以不会有并发安全问题

3、当然下面因为class CacheClient被倒导入的时候就生成了RedisClient, 所以无论多线程还是协程模式下都不会有并发安全问题

```python
# 分布式锁保证并发安全
 def get_with_lock(self):
        """
        并发安全, 减少数据库访问压力,应对缓存击穿
        """
        lock_key = f'lock_{self.cache_key}'
        for i in range(500):
            data = self.client.get_json_val(self.cache_key)
            if data is not None:
                return data
            elif self.client.acquire_lock(lock_key):
                try:
                    data = self._get_original_data()
                    if data is not None:
                        # 允许空Dict/空List; 需要保证数据更新清空缓存
                        self.client.set_json_val(
                            self.cache_key, data, ex=self.timeout
                        )
                    return data
                finally:
                    self.client.release_lock(lock_key)
            else:
                time.sleep(0.01)
        raise InternalCustomError(msg='Redis并发锁获取失败')

# 单例模式 保证一个副本只有一个
class Singleton(type):
    """(类名)单例, metaclass=Singleton"""

    def __init__(cls, *args, **kwargs):
        cls.__instance = {}
        super().__init__(*args, **kwargs)

    def __call__(cls, *args, **kwargs):
        if cls.__name__ not in cls.__instance:
            cls.__instance[cls.__name__] = super().__call__(*args, **kwargs)
        if hasattr(cls.__instance[cls.__name__], "init"):
            cls.__instance[cls.__name__].init(*args, **kwargs)
        return cls.__instance[cls.__name__]

class RedisClient(metaclass=Singleton):
    pass

class CacheClient:
    client = RedisClient()


```