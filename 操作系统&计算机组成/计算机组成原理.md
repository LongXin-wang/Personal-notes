- [硬件核心组成](#硬件核心组成)
  - [CPU - 寄存器](#cpu---寄存器)
  - [总线](#总线)
    - [32位和64位](#32位和64位)
  - [存储-内存/磁盘](#存储-内存磁盘)
    - [CPU Cache](#cpu-cache)
    - [CPU 缓存一致性](#cpu-缓存一致性)
      - [总线嗅探](#总线嗅探)
      - [MESI](#mesi)
        - [volatile - 避免从cache中读取](#volatile---避免从cache中读取)
    - [虚拟内存](#虚拟内存)
- [IO设备](#io设备)
  - [DMA - 协处理器芯片](#dma---协处理器芯片)
    - [kafka 用DMA](#kafka-用dma)

# 硬件核心组成

运算器+控制器+存储单元+输入输出

![](https://gitee.com/wanglongxin666/pictures/raw/master/img/202401261105840.png)

## CPU - 寄存器

CPU 内部还有一些组件，常见的有**寄存器、控制单元和逻辑运算单元**等。其中，控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。

CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。

常见的寄存器种类：

- *通用寄存器*，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。
- *程序计数器*，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。
- *指令寄存器*，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。

## 总线

总线是用于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种：

- *地址总线*，用于指定 CPU 将要操作的内存地址；
- *数据总线*，用于读写内存的数据；
- *控制总线*，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；

当 CPU 要读写内存数据的时候，一般需要通过下面这三个总线：

- 首先要通过「地址总线」来指定内存的地址；
- 然后通过「控制总线」控制是读或写命令；
- 最后通过「数据总线」来传输数据；

### 32位和64位

**处理数据的大小和寻址能力**

CPU 想要操作「内存地址」就需要「地址总线」：

如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种地址，所以 CPU 能操作的内存地址最大数量为 2（2^1）个（注意，不要理解成同时能操作 2 个内存地址）；

如果地址总线有 2 条，那么能表示 00、01、10、11 这四种地址，所以 CPU 能操作的内存地址最大数量为 4（2^2）个。

那么，想要 CPU 操作 4G 大的内存，那么就需要 32 条地址总线，因为 `2 ^ 32 = 4G`。

32位CPU，一次能处理的数据量大小是32位

如果用 32 位 CPU 去加和两个 64 位大小的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个高位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个高位的 32 位数字，最后再加上进位，就能算出结果了，可以发现 32 位 CPU 并不能一次性计算出加和两个 64 位数字的结果。



## 存储-内存/磁盘

### CPU Cache
CPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 Cache Line（缓存块）。

> 比如，有一个 int array[100] 的数组，当载入 array[0] 时，由于这个数组元素的大小在内存只占 4 字节，不足 64 字节，CPU 就会顺序加载数组元素到 array[15]，意味着 array[0]~array[15] 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，而不用再从内存中读取，大大提高了 CPU 读取数据的性能。

array[i][j] 执行时间比形式二 array[j][i] 快好几倍。顺序访问，会把紧跟的几个元素加载到cpu cahce中

> 要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率高的代码，CPU L1 Cache 分为数据缓存和指令缓存，因而需要分别提高它们的缓存命中率：

- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；
- 对于指令缓存，有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率；

![](https://gitee.com/wanglongxin666/pictures/raw/master/img/202401261113765.png)

### CPU 缓存一致性

和mysql的事物差不多，多核并行下对同一个记录的修改，怎么保证一致性

要保证做到下面这 2 点：

- 第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为**写传播（*****Write Propagation*****）**；
- 第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（*****Transaction Serialization*****）**

要实现事务串行化，要做到 2 点：

- CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；—`总线嗅探的工作机制`
- 要引入「锁」的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。—`MESI`

![](https://gitee.com/wanglongxin666/pictures/raw/master/img/202401261123916.png)

#### 总线嗅探

`我还是以前面的 i 变量例子来说明总线嗅探的工作机制，当 A 号 CPU 核心修改了 L1 Cache 中 i 变量的值，通过总线把这个事件广播通知给其他所有的核心，然后每个 CPU 核心都会监听总线上的广播事件，并检查是否有相同的数据在自己的 L1 Cache 里面，如果 B 号 CPU 核心的 L1 Cache 中有该数据，那么也需要把该数据更新到自己的 L1 Cache。`

可以发现，总线嗅探方法很简单， CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。

另外，总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串行化。

于是，有一个协议基于总线嗅探机制实现了事务串行化，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存一致性。

#### MESI

当一个处理器要读取或写入一个内存位置时，首先检查其他处理器的缓存状态。如果其他处理器有相同的内存位置且处于已修改、已独占或已共享状态，就需要根据协议执行相应的操作，以保证缓存的一致性。

当A号CPU要改cache中i的变量时，会将其它核的标记为无效状态（此时其它核不能修改i），然后A核更新i，串行化

##### volatile - 避免从cache中读取

**确保我们对于这个变量的读取和写入，都一定会同步到主内存里，而不是从 Cache 里面读取**

使用 volatile 修饰共享变量后，每个线程要操作变量时会从主内存中将变量拷贝到本地内存作为副本，当线程操作变量副本并写回主内存后，会通过 **CPU 总线嗅探机制**告知其他线程该变量副本已经失效，需要重新从主内存中读取。

### 虚拟内存

计算机系统内存管理的一种技术，允许执行进程不必完全处于内存

虚拟内存通过内存映射，映射到物理内存

CPU通过MMU（地址管理） 寻址

Swap 技术允许一部分进程使用内存，不使用内存的进程数据先保存在磁盘上。注意，这里提到的数据，是完整的进程数据，包括正文段（程序指令）、数据段、堆栈段等。轮到某个进程执行的时候，尝试为这个进程在内存中找到一块空闲的区域。如果空间不足，就考虑把没有在执行的进程交换（Swap）到磁盘上，把空间腾挪出来给需要的进程。

- Write-back: 直到这个缓存需要被置换出去，才写入到内存（需dirty bit表示缓存中数据是否和内存中相同，因为可能某时刻内存中对应地址的数据已更新，重复写入就会导致原有数据丢失）
- 分页存储管理：页号—>内存块号（映射），块号+页内偏移地址找到物理地址
- 分段存储管理：段号—>起始地址，段号—+段内偏移地址找到物理地址
- 段页式存储管理：先分配段，再分配页

缺页中断机制：请求某页， 页面是否在快表中，否的话，是否在内存中，否的话，缺页中断，请求调页 

内存是否已满，已满进入页面置换算法 - 最近最久未使用（LRU）


> "最近最久未被使用"（LRU，Least Recently Used）和"最近最少被使用"（LFU，Least Frequently Used）是两种不同的缓存淘汰算法。
> LRU算法是基于数据项最近被访问的时间来进行淘汰决策的，即最久未被使用的数据项会被淘汰。
> 而LFU算法是基于数据项被访问的频率来进行淘汰决策的，即最少被使用的数据项会被淘汰

# IO设备

## DMA - 协处理器芯片

对于 I/O 设备的大量操作，其实都只是把内存里面的数据，传输到 I/O 设备而已。特别是当传输的数据量比较大的时候，比如进行大文件复制，如果所有数据都要经过 CPU，实在是有点儿太浪费时间了。

 DMA 技术，也就是直接内存访问（Direct Memory Access）技术，来减少 CPU 等待的时间。一个协处理器

 其实就是CPU高速DMA要做哪些事情，交给DMA去做了，比如在IO设备之间传输数据，完全不需要CPU去等待处理，DMA可以将硬盘数据搬到内存

 ### kafka 用DMA

本来的过程：

在这个过程中，数据一共发生了四次传输的过程。其中两次是 DMA 的传输，另外两次，则是通过 CPU 控制的传输。下面我们来具体看看这个过程。

第一次传输，是从硬盘上，读到操作系统内核的缓冲区里。这个传输是通过 DMA 搬运的。

第二次传输，需要从内核缓冲区里面的数据，复制到我们应用分配的内存里面。这个传输是通过 CPU 搬运的。

第三次传输，要从我们应用的内存里面，再写到操作系统的 Socket 的缓冲区里面去。这个传输，还是由 CPU 搬运的。

最后一次传输，需要再从 Socket 的缓冲区里面，写到网卡的缓冲区里面去。这个传输又是通过 DMA 搬运的。

![](https://gitee.com/wanglongxin666/pictures/raw/master/img/202401261651100.png)

kafka的过程：

事实上，Kafka 做的事情就是，把这个数据搬运的次数，从上面的四次，变成了两次，并且只有 DMA 来进行数据搬运，而不需要 CPU。

在 Kafka 里，通过 Java 的 NIO 里面 FileChannel 的 transferTo 方法调用，我们可以不用把数据复制到我们应用程序的内存里面。通过 DMA 的方式，我们可以把数据从内存缓冲区直接写到网卡的缓冲区里面。在使用了这样的零拷贝的方法之后呢，我们传输同样数据的时间，可以缩减为原来的 1/3，相当于提升了 3 倍的吞吐率。

![](https://gitee.com/wanglongxin666/pictures/raw/master/img/202401261651585.png)

